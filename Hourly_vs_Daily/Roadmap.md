Below is a workflow that treats the **present (CTRL)** simulation as the *reference truth* and asks:

> *If the only thing that changes in the PGW run is the distribution of **daily** precipitation, what hourly distribution would we expect?  Does the model‐simulated hourly change fall within that expectation?*

Everything is framed as a statistical hypothesis test, so you can say “yes, compatible” or “no, there is an extra sub-daily signal”.

---

## 1.  Prepare consistent data sets

| Step                     | Why                                                                     | Practical tip                                                  |
| ------------------------ | ----------------------------------------------------------------------- | -------------------------------------------------------------- |
| Extract wet days/hours   | Avoid zeros diluting distributions; use a small threshold (e.g. 0.1 mm) | Keep a binary “wet” mask; you’ll need it later for resampling  |
| Aggregate hourly → daily | Creates matched pairs (P<sub>d</sub>, P<sub>h</sub>)                    | Store local time; Mediterranean convection is strongly diurnal |
| Match seasons or months  | PGW warming is often seasonally uneven                                  | Work separately for DJF, MAM, JJA, SON                         |

---

## 2.  Describe the CTRL **hourly–daily link**

You want the *conditional* distribution

$$
f\_{\text{CTRL}}\!\left(\frac{P_{h}}{P_{d}}\;\middle|\;P_{d}>0\right),
$$

i.e. how the 24 hourly totals share one day’s rain.

Two easy, non-parametric representations:

1. **Hourly-fraction vectors**
   For each wet day *j*, create

   $$
   \mathbf{q}^{(j)}=\bigl(q\_1,\dots ,q\_{24}\bigr)=\frac{\mathbf{P}^{(j)}\_{\text{hour}}}{P^{(j)}_{d}},\qquad \sum_{i=1}^{24}q_i=1 .
   $$

2. **Peak-hour ratio**
   $r^{(j)}=\max\_i P^{(j)}_{h,i} / P^{(j)}_{d}$.

Keep the full empirical sample; ten years gives you \~3 650 wet-day vectors—enough for bootstrap tests up to the 99 % quantile of $r$.

---

## 3.  Build a *null model* for PGW hours

Under the null hypothesis “sub-daily structure is unchanged”, an **expected PGW hourly series** can be generated by *resampling* CTRL hourly-fraction vectors and “pouring” each future day’s rain into them:

```
for each wet PGW day k:
    draw with replacement q* from {q(j)}CTRL
    P̂_h_i (k) = q*_i · P_d_PGW(k)   for i = 1,…,24
```

Do that N ≈ 500–1000 times to obtain a **bootstrap ensemble** of “CTRL-informed but PGW-scaled” hourly data.

This captures:

* CTRL sub-daily variability
* PGW daily totals (mean shift, variance change, seasonality)

Nothing else.

---

## 4.  Compare *simulated* vs *expected* PGW hourly distributions

Choose metrics that matter for your study; common choices:

| Metric              | Test statistic                   | Interpretation             |
| ------------------- | -------------------------------- | -------------------------- |
| All-hours CDF       | 2-sample KS or Anderson–Darling  | Whole distribution shift   |
| Wet-hour frequency  | Δ( #wet hours / #all hours )     | More drizzle vs dry spells |
| Extreme p-quantiles | Q<sub>99</sub>, Q<sub>99.9</sub> | Sub-daily intensification  |
| Peak-hour ratio     | ΔMedian(r), Δ95-th(r)            | Storm “concentration”      |

**Procedure**

1. For each bootstrap member compute the metric → get a sampling distribution.
2. Put the *real* PGW value on the same axis.
3. If the PGW value lies outside (say) the central 95 % of the bootstrap, reject the null at α = 0.05.

Because daily totals are already changed in the null, any additional PGW hourly shift that is *statistically significant* indicates a genuine change in sub-daily processes (convective organization, storm persistence, etc.).

---

## 5.  Quantile-scaling quick check (rule-of-thumb)

A fast, visual sanity test before the full bootstrap:

1. Compute change factors $C\_q = Q\_{q}^{\text{PGW}} / Q\_{q}^{\text{CTRL}}$ for

   * daily totals
   * hourly totals.

2. Plot $C\_q$ vs $q$ (e.g. 10 – 99.9 %).

If the two lines coincide within sampling error, sub-daily change is scale-invariant; large divergence (especially in the upper tail) flags disproportionate hourly intensification.

---

## 6.  Optional: Copula / parametric alternatives

* **Gaussian or Canonical Vine copula** of (P<sub>d</sub>, P<sub>h,max</sub>):
  fit on CTRL, sample with PGW daily marginal → same logic as above but smoother tails.

* **Multifractal scaling**:
  Fit intensity scaling
  $E[P(\tau)] \propto \tau^{H}$ (1 h → 24 h).
  A significant PGW change in slope *H* signals new intermittency.

These give the same answer but need more code.

---

## 7.  Practical considerations & pitfalls

* **10 years ≈ 87 600 hours**: Extreme hourly quantiles above 99.9 % are noisy; use block-bootstrap (24-h blocks) to preserve intra-day correlation.
* **Seasonality**: Mediterranean extremes are autumn-heavy; mixing seasons hides signals.
* **Spatial pooling**: If you want regional averages, apply the test at grid-cell level first, then map significance fractions (“field significance”).
* **Independence**: Daily sums are not independent across days in persistent events; a moving-blocks bootstrap (length 4–7 days) mitigates this.

---

## 8.  Reporting the answer

| Outcome                                                                       | Meaning                                                                                                                  |
| ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| PGW hourly metrics **inside** CTRL-based bootstrap envelope                   | Hourly changes are “compatible” with daily shifts. Nothing surprising.                                                   |
| PGW hourly metrics **outside** envelope (esp. extremes or wet-hour frequency) | There is an additional sub-daily signal—e.g. more intense short-lived bursts—that daily statistics alone cannot explain. |

Describe where (season, region, quantile) the divergence occurs and relate to physical mechanisms (stronger CAPE, warmer SSTs, etc.).

---

### Minimal Python sketch (pseudo-code)

```python
# Prepare CTRL arrays
Q_vectors = build_hourly_fraction_vectors(CTRL_hourly, CTRL_daily)
# Bootstrap expected PGW hours
exp_hourly = []
for _ in range(N_boot):
    exp_hourly.append(resample_and_scale(Q_vectors, PGW_daily))
# Compare distributions
ks_vals = [ks_2samp(pgw_hourly, e) for e in exp_hourly]
pval = (np.sum(ks_vals > ks_real) + 1) / (N_boot + 1)
```

---

**Bottom line:**
Use the present-day joint (hourly | daily) distribution as a *transfer function*; if the PGW hourly field deviates beyond sampling uncertainty, the model is producing genuinely new sub-daily behaviour, not just a scaled-up version of today’s rain.



---

### ✔️ Step 1 wrap-up

From what you describe you already have everything you need for the “data wrangling” part:

| variable              | dims                 | comment                                       |
| --------------------- | -------------------- | --------------------------------------------- |
| **wet\_days**         | (exp, *ndays*, y, x) | True/False mask of wet days                   |
| **wet\_days\_hourly** | (exp, *time*, y, x)  | True for the 24 h that belong to each wet day |
| **ds\_d**             | (exp, *ndays*, y, x) | daily totals (≤ 0.1 mm masked)                |
| **ds\_h**             | (exp, *time*,  y, x) | hourly totals (≤ 0.1 mm masked)               |

That is all we need for step 2.

---

## 2  Describe the CTRL hourly–daily link

Below is **fully vectorised xarray code** that turns the present-day data into

* **`q`** – the 24-element hourly-fraction vector for every wet day & grid point
  (shape =`(day, hour, y, x)`; each vector sums to 1)
* **`r`** – the “peak-hour ratio” (max-hour / daily) for every wet day & grid point
  (shape =`(day, y, x)`)

You can plug this straight into a notebook; adapt variable names if yours differ.

```python
import xarray as xr
import numpy as np

# ---- pick the present-day experiment ----
CTRL = 'Present'
h          = ds_h.sel  (exp=CTRL)          # (time, y, x)
d          = ds_d.sel  (exp=CTRL)          # (day , y, x)
mask_day   = wet_days.sel        (exp=CTRL)
mask_hour  = wet_days_hourly.sel (exp=CTRL)

# ---- map each hourly stamp to its parent day ----
# (assumes h.time is strictly regular, exactly 24 h per day)
day_index = xr.DataArray(
    np.repeat(np.arange(d.sizes['day']), 24),
    dims   = ['time'],
    coords = {'time': h['time']}
)

# broadcast the matching daily total onto every hour
d_broadcast = d.isel(day=day_index).drop_vars('day')   # (time,y,x)

# ---- hourly fraction q = P_h / P_d ----
q_hourly = xr.where(mask_hour, h / d_broadcast, np.nan)


# --- build named 1-D coordinate arrays ---------------------------------
day_coord  = day_index.rename('day')              # give it a name!
hour_coord = q_hourly['time'].dt.hour.rename('hour')


# add explicit day & hour coordinates **without any arithmetic on datetimes**
# --- attach the coordinates safely -------------------------------------
q_indexed = q_hourly.assign_coords(
    day  = ('time', day_coord.data),              # or just day_coord if you prefer
    hour = ('time', hour_coord.data)              # same idea
)

# reshape from flat time → (day, hour)
q = (
    q_indexed
    .set_index(time=['day', 'hour'])              # composite index
    .unstack('time')                              # → separate dims
    .transpose('day', 'hour', 'y', 'x')
)

# ---- peak-hour ratio r ----
r = q.max('hour')                            # (day, y, x)

# ---- apply wet-day mask to keep only valid samples ----
q = q.where(mask_day)
r = r.where(mask_day)
```

## A few notes on what the code is doing

| Step                                     | Why this way?                                                                              |
| ---------------------------------------- | ------------------------------------------------------------------------------------------ |
| **Broadcast with `isel(day=day_index)`** | avoids an expensive `.groupby('time.day')` loop and works lazily with dask.                |
| **Explicit `hour` coordinate**           | gives you a clean `(day, hour, y, x)` array that is easy to sample from later.             |
| **`mask_day` after computing**           | guarantees vectors still sum to 1 while marking non-wet days as NaN.                       |
| **NaNs outside wet hours**               | makes later statistics automatically ignore dry hours (xarray/numpy nan-aware reductions). |

---

### What you have now

| variable   | dims                  | purpose in later steps                     |
| ---------- | --------------------- | ------------------------------------------ |
| `q`        | (day, **hour**, y, x) | empirical distribution of hourly fractions |
| `r`        | (day, y, x)           | diagnostic for storm concentration         |
| `mask_day` | (day, y, x)           | lets you choose wet-day samples quickly    |

These *are* the “link” between hourly and daily rainfall for the present climate.
Everything else (bootstrap resampling, extreme tests, etc.) simply **draws rows of `q`** and “pours” PGW daily totals through them.

---

### Ready for step 3?

If this matches what you see on disk, the next move is:

1. Load the Future daily field (`ds_d.sel(exp='Future')`).
2. Write the bootstrap loop that, for each future day, samples one `q` row and multiplies it by that day’s total.

Let me know if you’d like the bootstrap code fleshed out, tips on memory-safe implementation with dask, or ideas for specific diagnostics (KS test, quantile shift, field significance maps, …).


### What are **`q`** and **`r`** and why bother?

| symbol                   | dimensionality                                     | meaning                                                                                                                                                     | what we will *do* with it                                                                                                                                                                                                                                                                                                     |
| ------------------------ | -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`q(day, hour, y, x)`** | 4-D, one value per wet day, clock hour, grid point | **Hourly-fraction vector** for that day:  <br>\\(q\_{h}=P\_{h}/P\_{d}\\).  For every wet day the 24 numbers sum to 1.                                       | *Building block for the null model.*<br>In the bootstrap we randomly pick one of these present-day vectors and “pour” each **Future** day’s total rain through it.  That creates the **expected** hourly PGW series *if nothing but daily totals changed*.                                                                    |
| **`r(day, y, x)`**       | 3-D, one value per wet day, grid point             | **Peak-hour ratio**:  <br>\\(r=\displaystyle\max\_{h}P\_{h}/P\_{d}\\).  A perfect 24 h drizzle has \\(r=1/24≈0.04\\); a violent 1-hour burst has \\(r→1\\). | *Diagnostic to compare concentration.*<br>We will look at the distribution of **`r`**:<br>• present-day CTRL<br>• CTRL-based bootstrap of Future<br>• actual Future simulation<br>If the Future run’s \\(r\\)-quantiles lie outside the bootstrap envelope, hourly extremes intensified more than daily totals alone explain. |

---

#### Why not just keep the raw hourly data?

* We need a **conditional** description *given the daily total*.
  `q` does exactly that: it says *how* the daily rainfall is split among the 24 hours.
* With `q` we can generate millions of synthetic “CTRL-behaving” hour-series cheaply and lazily (just one multiply), which is ideal for a bootstrap.
* `r` is a *one-number* summary that is extremely sensitive to sub-daily intensification but costs almost no memory, so we can map or plot it easily.

---

### How they fit in the full workflow

1. **Present climate → `q` and `r`**
   *Establish the CTRL hourly-daily relationship.*
2. **Bootstrap expected Future hours**
   *For each future wet day*:
     `P̂_h = q_sample × P_d_future`
   `→` ensemble of “expected” hourly totals.
3. **Compare metrics**
   *On `r` (or any other statistic you like)*

   * 2-sample KS or quantile comparison between
     – actual PGW hourly data
     – bootstrap ensemble based on `q`.
   * *If* they match ⇒ hourly change is “compatible” with daily;
     *else* ⇒ extra sub-daily signal.
4. **Visualisation / mapping**
   Because `r` is tiny (one value per day), you can compute its 95-th percentile per grid point and plot **Δr** maps without ever materialising the monster `(time,y,x)` array.

---

### Quick mental picture

```
CTRL wet day j
  daily total P_d  ─┐
                   ├──► q(j) = [0.02, 0.05, … 0.07]
                   │
                   └──► r(j) = 0.22   (peak / daily)

Future wet day k   P_d = 40 mm
  pick random q(j)  ─┐
                     ├──► expected hourly: 40·q(j)
                     └──► compare to simulated PGW hours
```

So `q` is the **template**; `r` is a **stress-test metric**.
If that still feels abstract, tell me which part of the subsequent analysis you’d like spelled out (e.g. the exact bootstrap loop, KS test code, memory-safe quantile maps), and we’ll zoom in on that next.


Great — you now have the two key ingredients:

* **`rainfall_ratios`** (your *q*): hourly-fraction vectors for every wet day
  `dims = (day, hour, y, x)`  each 24-hour slice sums to 1

* **`ds_h_masked_max`** (your *r*): peak-hour ratios for those same wet days
  `dims = (day, y, x)`

The rest of the analysis is essentially three blocks:

| Block                                       | Goal                                                                                                                                        | Typical output                                           |
| ------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------- |
| **A. Build “expected PGW hours”**           | Generate an ensemble of hourly fields that keep CTRL sub-daily structure but scale them by Future daily totals.                             | 500–1000 synthetic hourly datasets (still lazy/on-disk). |
| **B. Compute metrics**                      | For each ensemble member and for the actual PGW run, compute things you care about (CDFs, quantiles, KS distance, new peak-hour ratios, …). | Small arrays (or single numbers) you can pull into RAM.  |
| **C. Decide “compatible / not compatible”** | Compare the PGW metric to the bootstrap distribution.                                                                                       | p-values, significance maps, plots.                      |

Below is code skeleton (xarray + dask-friendly, float32, memory-safe) that you can drop into a notebook or script and adapt to your variable names and file paths.

---

## A.  Bootstrap “expected” hourly PGW fields

```python
import xarray as xr, numpy as np, dask.array as da, dask

# ------------- open CTRL templates & PGW daily totals -----------------
q_ctrl  = rainfall_ratios            # (day, hour, y, x)  float32
r_ctrl  = ds_h_masked_max            # (day, y, x)
daily_pgw = ds_d.sel(exp='Future')   # (day, y, x) masked ≤0.1 mm

# guarantee equal number of days
nday = min(q_ctrl.sizes['day'], daily_pgw.sizes['day'])
q_ctrl  = q_ctrl .isel(day=slice(0, nday))
daily_pgw = daily_pgw.isel(day=slice(0, nday))

# ------------- helper: broadcast PGW daily totals onto hours ----------
daily_pgw_hour = (
    daily_pgw
      .rename({'day': 'hour'})               # fake dim
      .reindex(hour=q_ctrl.hour, method='ffill')  # repeats each total 24×
      .transpose('day', 'hour', 'y', 'x')
)

# ------------- bootstrap loop (lazy) ----------------------------------
NBOOT   = 500                # increase if you can afford more time
rng     = da.random.RandomState(seed=123)

boot_metrics = []
for b in range(NBOOT):
    # sample day-wise with replacement along first dimension
    day_idx = rng.randint(0, nday, size=nday, chunks=q_ctrl.chunks['day'])
    q_samp  = q_ctrl.isel(day=day_idx)

    # expected hourly field for this bootstrap draw
    p_exp_h = q_samp * daily_pgw_hour         # still lazy

    # ---- example metric: 99-th quantile of hourly rain ---------------
    q99 = p_exp_h.quantile(0.99, dim=('day', 'hour'))
    boot_metrics.append(q99)

boot_metrics = xr.concat(boot_metrics, dim='boot')  # (boot, y, x)
```

*Everything above is lazy; nothing heavy is in RAM until you call* `boot_metrics.compute()` *or similar.*

---

## B.  Compute the same metric for the real PGW hours

```python
h_pgw = ds_h.sel(exp='Future')        # (time, y, x) masked ≤0.1 mm
q99_pgw = h_pgw.quantile(0.99, dim='time')   # (y, x)
```

---

## C.  Compare & flag significance

```python
# 95 % two-sided envelope from the bootstrap
lower = boot_metrics.quantile(0.025, dim='boot')
upper = boot_metrics.quantile(0.975, dim='boot')

signif_mask = (q99_pgw < lower) | (q99_pgw > upper)   # (y, x) boolean

# Optional: a p-value field
pvals = (xr.concat([boot_metrics, q99_pgw.expand_dims(boot=1)], dim='boot')
           .rank(dim='boot')[-1] / (NBOOT+1))
```

Now you have tiny arrays (`upper`, `lower`, `q99_pgw`, `signif_mask`, `pvals`) that fit easily in memory and can be plotted or summarised.

---

### Typical follow-ups

| If you want …                                                    | Do this                                                                                                                                                         |
| ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Maps** of where hourly extremes intensified beyond expectation | plot `signif_mask` or `pvals` on a Mediterranean map.                                                                                                           |
| **Field significance** (Wilks 2006)                              | count how many grid cells are significant; compare to binomial α.                                                                                               |
| **CDF or KS test** of all hours                                  | replace the quantile metric with `cdf = p_exp_h.stack(t=('day','hour'))` and run `scipy.stats.ks_2samp` on small random subsamples (or dask-map across chunks). |
| **Storm-concentration test**                                     | repeat the same workflow but with your `r_ctrl` instead of `q_ctrl`: build a bootstrap distribution of future-expected `r` and compare to actual PGW `r`.       |
| **Seasonal focus**                                               | slice your datasets before the loop: `q_ctrl = q_ctrl.sel(day=season_index)` and same for `daily_pgw`.                                                          |

---

### One-line interpretation template

> “At 51 × 51 grid points the 99-th hourly rainfall percentile in the PGW run exceeds the CTRL-based expectation at **14 %** of locations (vs. 5 % expected by chance, field-significant at α = 0.05), mainly over the northern Adriatic and coastal Catalonia. Hence sub-daily intensification cannot be explained by daily scaling alone.”

---

Let me know which specific diagnostic, plot, or robustness test you’d like fleshed out next — or if you hit any snags wiring your exact filenames and chunk sizes into the skeleton above!
